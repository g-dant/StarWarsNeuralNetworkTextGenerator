{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mimicking Star Wars Characters Using Neural Networks\n\nHello! The aim of this NoteBook is to implement different Neural Networks capable of generating dialogue texts of different Star Wars characters. We will implement it using the Keras library and the Tensorflow backend.\n\n![IMG](https://media.giphy.com/media/3ornk57KwDXf81rjWM/giphy.gif)\n\nWell, first things first: let's import our libraries:"},{"metadata":{"_execution_state":"idle","_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","trusted":true},"cell_type":"code","source":"## Importing packages\n\n# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n# You can see which packages are installed by checking out the kaggle/rstats docker image: \n# https://github.com/kaggle/docker-rstats\n\noptions(warn = -1)\n\nlibrary(wordcloud) # Word Clouds\nlibrary(jpeg) # Read .JPEG\nlibrary(circlize) # Chord Diagrams\nlibrary(png) # Read .PNG\nlibrary(RCurl) # Get contents from URLs\nlibrary(RColorBrewer) # Color Scheme to the Word Cloud\nlibrary(scales) # Custom scales to Ggplot\nlibrary(repr) # Resize R plots in Jupyter Notebooks\nlibrary(tidyverse) # Metapackage with lots of helpful functions\nlibrary(tensorflow) # Deep learning backend\nlibrary(kerasR) # Use Keras' Early Stopping\nlibrary(caret) # Use in K-Fold split\nlibrary(keras) # API that interacts with tensorflow\nlibrary(gridExtra) # Multiple ggplots same figure\nlibrary(magrittr) # Enable pipe operator\n\noptions(warn = 0)\n\n## Running code\n\n# In a notebook, you can run a single code cell by clicking in the cell and then hitting \n# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, \n# you can run code by highlighting the code you want to run and then clicking the blue arrow\n# at the bottom of this window.\n\n## Reading in files\n\n# You can access files from datasets you've added to this kernel in the \"../input/\" directory.\n# You can see the files added to this kernel by running the code below. \n\nlist.files(path = \"../input\")\n\n## Saving data\n\n# If you save any files or images, these will be put in the \"output\" directory. You \n# can see the output directory by committing and running your kernel (using the \n# Commit & Run button) and then checking out the compiled version of your kernel.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input.dir <- '/kaggle/input/star-wars-movie-scripts/'\ninput.dir <- '/kaggle/input/star-wars-movie-scripts/'\noptions(repr.plot.width=8)\nSys.setenv(RETICULATE_PYTHON = \"/usr/local/share/.virtualenvs/r-reticulate/bin/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The basic steps we will follow is to:\n1. Analyse the Dataset\n2. Prepare the Data\n3. Implement the Models\n4. Test the Models\n5. Take Conclusions\n\n# 1A. Data Analysis - PART I: Checking the File\nLet's start by taking a look at the input data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"filterText <- function(text.in) {\n    strsplit(gsub('[^[:alnum:] \\']|?|!', '', text.in) %>% tolower, ' ') %>%\n    unlist %>% (function(X) X[X != '']) %>% return\n}\n\nformatScriptText <- function(file.name) {\n    \n    read.delim(paste(c(input.dir, file.name), collapse = ''), stringsAsFactors = FALSE) %>% \n\n    mutate(words.list = lapply(character.dialogue, filterText)) %>% \n    mutate(ID = lapply(words.list, (function(X) X[1]))) %>%\n    mutate(Character = lapply(words.list, (function(X) X[2])) %>% unlist) %>%\n    mutate(Words.List = lapply(words.list, (function(X) tail(X, -2)))) %>%\n                                            \n    select(c('ID', 'Character', 'Words.List')) %>% return\n}\n                                            \n'SW_EpisodeIV.txt' %>% formatScriptText %>% head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a text file and we can get the ID of the dialogue text, the character name and the text itself by processing the generated text, which is represented by a 1-column dataframe:"},{"metadata":{},"cell_type":"markdown","source":"Then we can put the dataframe of each episode on the same structure and add a column to show the episode of eatch dialogue text:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ep4 <- formatScriptText('SW_EpisodeIV.txt')\ndf_ep5 <- formatScriptText('SW_EpisodeV.txt')\ndf_ep6 <- formatScriptText('SW_EpisodeVI.txt')\n\ndf_ep4$Episode <- '4'\ndf_ep5$Episode <- '5'\ndf_ep6$Episode <- '6'\n\ndf_all <- bind_rows(bind_rows(df_ep4, df_ep5), df_ep6)\nhead(df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a movie, some characters never appears and we need to have a nice amount of words to train a good model, so we can get all the characters that appeared more than a minimum number of times:"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_appearance = 100\ndf_main_characters <- df_all %>% \n                      group_by(Character) %>% \n                      mutate(count = n()) %>%\n                      filter(count > min_appearance)\nggplot(df_main_characters, aes(x = Character, fill = Character)) + \n    geom_bar(stat = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a **HUGE** problem here: where is Yoda? In our model should Yoda be.\n\n![YODA](https://media.giphy.com/media/3ohuAxV0DfcLTxVh6w/giphy.gif)\n\nSo, we are going to add an extra condition to our filter: if the Character equals Yoda, then the row will be considered in our analysis!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main_characters <- df_all %>% \n                      group_by(Character) %>% \n                      mutate(count = n()) %>%\n                      filter(count > min_appearance | Character == 'yoda') %>%\n                      select(-c('count'))\n\nggplot(df_main_characters, aes(x = Character, fill = Character)) + geom_bar(stat = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better! Actually we are not interested in taking the number of appearances, we want the number of spoken words. But the number of appearances is a proxy of this quantity as we can check below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main_characters <- df_main_characters %>% mutate(N.Words = lapply(Words.List, (function(X) length(X))) %>% unlist)\ndf_main_characters_statistics <- df_main_characters %>% \n                                 group_by(Character) %>% \n                                 summarise(Total.Words = sum(N.Words), N.Appearances = n()) %>%\n                                 mutate(Words.Per.Appearance = Total.Words / N.Appearances) %>%\n                                 as.data.frame\n                                                                                  \ndf_main_characters_statistics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Words / Appearances doesn't seem to change much. Also, the Total.Words seems to follow the N.Appearances variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"gg1 <- df_main_characters_statistics %>% ggplot(aes(x = Character, y = N.Appearances, fill = Character)) + geom_bar(stat = 'identity')\ngg2 <- df_main_characters_statistics %>% ggplot(aes(x = Character, y = Total.Words, fill = Character)) + geom_bar(stat = 'identity')\ngg3 <- df_main_characters %>% ggplot(aes(x = Character, y = N.Words, fill = Character)) + \n                                geom_jitter(aes(alpha = 0.9999999)) + geom_boxplot() + scale_y_continuous(trans = log2_trans())\n\ngg1 <- gg1 + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank())\ngg2 <- gg2 + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank())\ngg3 <- gg3 + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\noptions(repr.plot.width = 18)\ngrid.arrange(gg1, gg2, gg3, ncol = 3)\noptions(repr.plot.width = 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, the number of words seems to be strongly correlated with the number of appearances. We can check it by taking the Pearson correlation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all <- df_all %>% mutate(N.Words = lapply(Words.List, (function(X) length(X))) %>% unlist)\ndf_all_statistics <- df_all %>% group_by(Character) %>% summarise(Total.Words = sum(N.Words), N.Appearances = n()) %>%\n                     mutate(Words.Per.Appearance = Total.Words / N.Appearances) %>% as.data.frame\n                                                                      \ncor(x = df_all_statistics$Total.Words, y = df_all_statistics$N.Appearances, method = 'pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation is $98.6 \\%$. So we are following the right way!"},{"metadata":{},"cell_type":"markdown","source":"# 1.B. Data Analysis - PART II: Checking the Data"},{"metadata":{},"cell_type":"markdown","source":"What about the content of the dialogues? Well, we take a general browse on the dialogues using a word cloud diagram, segregated by Characters and by Star Wars Episodes! It will be useful to compare the generated texts with each cloud and, also, we can learn a little bit more about the Star Wars good and old trilogy!\n\nWe will, then, show as an example the evolution of Luke for each episode...later, during the testing step, we will check the cloud words with the output of each Neural Network..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_per_character_and_ep <- df_main_characters %>% \n                           select(c('Character', 'Episode', 'Words.List')) %>%\n                           group_by(.dots = c('Character', 'Episode')) %>%\n                           summarise(Full.Text = Reduce(c, Words.List) %>% paste(collapse = ' ')) %>% \n                           as.data.frame\n\ndf_per_character <- df_main_characters %>% \n                    select(c('Character', 'Episode', 'Words.List')) %>%\n                    group_by(.dots = c('Character')) %>%\n                    summarise(Full.Text = Reduce(c, Words.List) %>% paste(collapse = ' ')) %>% \n                    as.data.frame\n\ndf_per_ep <- df_main_characters %>% \n             select(c('Character', 'Episode', 'Words.List')) %>%\n             group_by(.dots = c('Episode')) %>%\n             summarise(Full.Text = Reduce(c, Words.List) %>% paste(collapse = ' ')) %>% \n             as.data.frame\n\ncharacters_list <- unique(df_per_character_and_ep$Character)\nepisodes_list <- unique(df_per_character_and_ep$Episode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n.Characters <- nrow(df_per_character)\nprint.Word.Cloud <- function(character_name = 'ALL', ep = 'ALL'){\n    if (ep == 'ALL') {\n        df_filter <- df_per_character %>% filter(Character == character_name)\n    } else if (character_name == 'ALL') {\n        df_filter <- df_per_ep %>% filter(Episode == ep)\n    } else {\n        df_filter <- df_per_character_and_ep %>% filter(Character == character_name, \n                                                        Episode == ep)\n    }\n    wordcloud(df_filter$Full.Text[1], max.words = 100, min.freq = 0, colors = brewer.pal(8, \"Dark2\"))\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.B1 - Luke on Ep. IV - Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"print.Word.Cloud('luke', '4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems that Luke is really concerned with C3PO on the episode IV"},{"metadata":{},"cell_type":"markdown","source":"## 1.B2 - Luke on Ep. V - Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"print.Word.Cloud('luke', '5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this Episode, Yoda appears and R2D2 (artoo) is repaired...so, it does make sense!"},{"metadata":{},"cell_type":"markdown","source":"## 1.B3 - Luke on Ep. VI - Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"print.Word.Cloud('luke', '6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes! In this episode Luke discovers, effectively, that Darth Vader is **HIS FATHER**! And then, FATHER is just the most spoken Luke's word! (After filtering words like articles or prepositions).\n\n![YAY](https://i0.wp.com/www.meugamer.com/wp-content/uploads/2018/05/luke-i-am-your-father-meugamer.jpg?fit=800%2C450&ssl=1)"},{"metadata":{},"cell_type":"markdown","source":"## 1.C. Check Mutual Character Relationships\nWould it be possible to find a measure to check mutual relationships among different characters? Sure! We can, for instance, count how many times one character says the name of other character and plot it in a chord diagram and that's what we are going to do in this section."},{"metadata":{},"cell_type":"markdown","source":"### 1.C.1 Mentions Diagram - All Episodes"},{"metadata":{"trusted":true},"cell_type":"code","source":"get.Mentions.Number <- function(from_character, to_character, ep='ALL') {\n    if (ep == 'ALL') {\n        df_filter <- df_per_character %>% filter(Character == from_character)\n    } else {\n        df_filter <- df_per_character_and_ep %>% filter(Character == from_character, \n                                                        Episode == ep)\n    }\n    out <- if (length(df_filter$Full.Text) > 0) (str_count(df_filter$Full.Text[[1]], to_character) %>% sum) else (0)\n    return(out)\n}\n\nplot.Mentions = function(ep='ALL') {\n    character_list <- df_per_character[['Character']] %>% unique\n    adj_mention_matrix <- matrix(0, character_list %>% length, character_list %>% length)\n\n    rownames(adj_mention_matrix) <- character_list\n    colnames(adj_mention_matrix) <- character_list\n\n    for (character1 in character_list) {\n        for (character2 in character_list) {\n            if (character1 != character2) {\n                adj_mention_matrix[[character1, character2]] <- get.Mentions.Number(character1, character2, ep)\n            }\n        }\n    }\n    \n    par(cex=2.5)\n    return(\n        adj_mention_matrix %>% chordDiagram(annotationTrack = c(\"name\", \"grid\"), \n                                        annotationTrackHeight = c(0.03, 0.01))\n    )\n}\n\nplot.Mentions()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also check the mentions per episodes:\n\n### 1.C.2. Mentions Diagram - Episode IV"},{"metadata":{},"cell_type":"markdown","source":"On the episode IV, we have few mentions. We can notice a strong relation between Luke and Threepio and between Luke and Han. Also, there is no Yoda appearance in this Episode."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot.Mentions(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.C.3. Mentions Diagram - Episode V"},{"metadata":{},"cell_type":"markdown","source":"The relations become more complex and it seems that everybody is aways mentioning Luke Skywalker. We can notice that this feature interesting to detect the protagonist of different stories, for instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot.Mentions(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.C.4. Mentions Diagram - Episode VI"},{"metadata":{},"cell_type":"markdown","source":"A strong relation between Luke Skywalker and Han Solo can be noticed on episode IV."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot.Mentions(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# 2. Data Preparation\n"},{"metadata":{},"cell_type":"markdown","source":"To train the neural network, we need to encode each word with a different number and organize them in windows, after converting the words list column to a tensor of words. By looking at the boxplot, it seems that a Window with size 5 is interesting - it's not too small and we can get more than 1 window with most of the dialogue texts.\n\nIf we have less words than 5 in a dialogue text, we will pad it with \"empty\" tokens. In this case, we will have \"$5 - N_{Words}$\" tokens that need to be appended to the tensor. We also have to remove non significant chars and words from the list before doing all this work."},{"metadata":{"trusted":true},"cell_type":"code","source":"all.words <- reduce(df_all['Words.List'], c)\ntokenizer <- text_tokenizer()\ntokenizer$fit_on_texts(all.words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check an example of coded window:"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts_to_sequences(tokenizer, \n                   c('much', 'to', 'learn', 'you', 'still', 'have')) %>% unlist %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we can create a dictionary mapping each found code to a different word:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all.words.unique <- all.words %>% unlist %>% unique\nwords.token <- texts_to_sequences(tokenizer, all.words.unique) %>% unlist\n\ntoken.Dict <- data.frame(Word = all.words.unique, Code = words.token) %>% arrange(Code)\ntoken.Dict %>% head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each sentence we get sliding windows of size $6$ with a slide step equal to $1$. The windows will be the inputs of our model\n![Windows](https://i2.wp.com/techieme.in/wp-content/uploads/sliding1.png)\n\nThe function that implements it is shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"take.Windows <- function(dialogue, tokenizer, window.size = 6, step = 1, split = T) {\n    text <- if (split) (dialogue %>% strsplit(split = ' ') %>% unlist) else (dialogue)\n    text <- texts_to_sequences(tokenizer, text)\n    map(seq(1, length(text) - window.size, by = step), ~text[.x:(.x + window.size)]) %>% return\n}\n\ntest <- 'much to learn you still have my jedi warrior' %>% take.Windows(tokenizer = tokenizer)\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a Recurrent Neural Network to predict the next word. So, the last element of the window will be labels using during the Training/Validation steps. Writting a function to separe the $X$ and $Y$ arguments of the fit method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"get.Character.Windows <- function(character.Name, tokenizer, window.size = 6) {\n    full.window <-  take.Windows(dialogue = (df_per_character %>% filter(Character == character.Name))[1, 'Full.Text'], tokenizer = tokenizer)             \n    \n    X_out <- lapply(full.window, function(X)(X[1:(window.size - 1)]))\n    Y_out <- lapply(full.window, function(X)(X[2:window.size]))\n                    \n    to.Mat <- function(X) {\n        do.call(rbind, lapply(X, rbind)) %>% return\n    }\n                    \n    to.Array <- function(X, add_dim) {\n        final_dim = c(nrow(X), ncol(X))\n        array(unlist(X), final_dim) %>% return\n    }\n                    \n    return(list(X = X_out %>% to.Mat %>% to.Array(add_dim = T), \n                Y = Y_out %>% to.Mat %>% to.Array(add_dim = F)))\n}\n                    \n('yoda' %>% get.Character.Windows(tokenizer = tokenizer))$X %>% dim\n('yoda' %>% get.Character.Windows(tokenizer = tokenizer))$Y %>% dim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Creating and Tunning the Model"},{"metadata":{},"cell_type":"markdown","source":"Recurrent Neural Networks (RNN's) are characterized by feedback connections that implements memory functions to the model. \n\n![RNN](https://i.stack.imgur.com/afqRj.png)\n\nWe transform the neural network into a common feedfoward one using the unfold concept: we repeat the neural network $N_{Window}$ times, where $N_{Window}$ is the number of elements on the training window (in our case: 5). It's almost like a feedfoward neural network but we have the a new restriction to the weights: they must be the same over each step of time obtained on the unfold procedure.\n\nIt's interesting to use the memory functionality here since the next element of a text depends on the last spoken words. We will train a different model to each character because of the different language styles that we can find. While Luke may probabily speak english using the normal order of words, Yoda may using more inversions speak ;)\n\nRNN's can be easily created using the Keras library as we can check below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen.Model <- function(n_units = 5, \n                      do_rate = 0.2, \n                      max_id = length(tokenizer$word_index), \n                      batch_size = 32,\n                      window_size = 5){\n    \n    model <- keras_model_sequential() %>%\n    \n        layer_gru(units = n_units,\n                  return_sequences = T,\n                  input_shape = c(window_size, 1),\n                  dropout = do_rate,\n                  recurrent_dropout = do_rate) %>%\n    \n        layer_gru(units = n_units, \n                  return_sequences = T, \n                  dropout = do_rate,\n                  recurrent_dropout = do_rate) %>%\n    \n        time_distributed(layer = \n                  layer_dense(units = max_id, activation = 'softmax')) %>% \n        \n        compile(optimizer = 'adam', loss = 'categorical_crossentropy') %>%\n    \n        return\n}\n\ngen.Model() %>% summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GRU layer is a \"Gated Recurrent Unit\". It can be seen as a simplification of the LSTM (Long Short Term Memory), which was developped to implement both: long-term memory and short-term memory over recurrent neurons with a different scheme of processing. The GRU is simpler and can also reach interesting results:\n\n![GRU X LSTM](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n.Epochs <- 100\nbatch.Size <- 32\n\ndataset <- get.Character.Windows('han', tokenizer)\nmodel <- gen.Model()\nmodel %>% summary\nX_in <- dataset$X\nY_in <- to_categorical(dataset$Y - 1, num_classes = length(tokenizer$word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_in %>% dim %>% print\nY_in %>% dim %>% print\n\nlength(tokenizer$word_index) %>% print","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history <- model %>% fit(x = X_in[1:32,] %>% array_reshape(c(32, 5, 1)), y = Y_in[1:32,,],\n                         batch_size = batch.Size, epochs = n.Epochs, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output of the softmax function (which is a generalization of the logistic model) is a probability distribution of output words."},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test <- model %>% predict(X_in[1:10,] %>% array_reshape(c(10, 5, 1)) %>% k_cast(dtype='float32'))\npredict_test %>% dim %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the sum of its elements must be equal to $1$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test[1, 1,] %>% drop %>% sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are ready to write a function to generate the desired texts and apply it to each selected character."},{"metadata":{},"cell_type":"markdown","source":"# 4. Generate Text from Model"},{"metadata":{},"cell_type":"markdown","source":"Let's try to complete the Yoda sentence: \"Much to learn you still...\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"strsplit('much to learn you still', split = ' ') %>% unlist %>% length","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The used function outputs the next word. It will be randomly taken over the output distribution. Before taking the next word, we will just adjust the output: the \"temperature\" parameter can be seen as the tendence to take different words. If the temperature is too small, we will just take the most probable word. On the other side, if the temperature is too high, we will take even the less probable function.\n\nSo, the temperature is a hyperparameter that must be tunned. Let $Y_k$ be the output of the Softmax function for the word index $k$. The temperature $T$ will re-scale it by doing:\n\n$Y'_k = e^{ \\frac{ log(Y_k) }{ T } }$\n\n$P_k = \\frac{Y'_k}{\\sum_{i = 1}^{N}{Y'_i}} $"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_next_word <- function(model, initial.Window, temperature = 1, verbose = 0){\n    \n    seq.Tokens <- texts_to_sequences(tokenizer, initial.Window)\n    n.Words <- length(seq.Tokens %>% unlist)\n    \n    X.in <- seq.Tokens %>% array_reshape(c(1, n.Words, 1))\n    model.Prediction <- (model %>% predict(X.in))[, 5,] %>% drop\n    \n    adjusted.Distribution <- exp(log(model.Prediction) / temperature)\n    adjusted.Distribution <- adjusted.Distribution / sum(adjusted.Distribution)\n    \n    rand.n <- runif(1, 0, 1)\n    cdf <- 0\n    index <- 0\n    \n    while(cdf < rand.n){\n        index <- index + 1\n        prob <- adjusted.Distribution[[index]]\n        cdf <- cdf + prob\n    }\n    \n    if (verbose > 0) { \n        barplot(adjusted.Distribution[1:100] %>% sort(decreasing = T),\n                main='Output Words Distribution',\n                xlab='Word Token',\n                ylab='Probability',\n                border='black',\n                col='blue',\n                density=10) \n    }\n    \n    output.Row <- token.Dict %>% filter(Code == index)\n    return(output.Row$Word[1] %>% as.character)\n}\n\n# Text to test: 'much to learn you still' -- Code: 149, 4, 375, 2, 269\nprint('Generated Word:')\ntest.Out.Word <- get_next_word(model, 'much to learn you still', 0.5, 1)\ntest.Out.Word %>% print\n\nprint('Generated Sentence:')\npaste('Much to learn you still [', test.Out.Word, ']') %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A verbose different of zero makes our function plot the distribution of output tokens after the temperature correction. The name \"temperature\" is given because of the Boltzmann equation of statistical physics:\n\n![Boltzmann Equation](http://spiff.rit.edu/classes/phys440/lectures/boltz/eqn_boltz.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen.Text <- function(initial.Window, model, n.Words = 8, temperature = 2) {\n    ans <- initial.Window\n    last.Word <- ''\n    for (i in 1:n.Words) {\n        words.list <- initial.Window %>% strsplit(split = ' ') %>% unlist\n        next.Word <- get_next_word(model, initial.Window, temperature = temperature)\n        if (last.Word != next.Word) { ans <- paste(ans, next.Word, collapse = ' ')  }\n        initial.Window <- paste(c(words.list[2:length(words.list)] %>% unlist, c(next.Word)), collapse = ' ')\n        last.Word <- next.Word\n    }\n    return(ans)\n}\n\ngen.Text('hello i\\'m a person that', model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Train the Model for Each Character and Check :)"},{"metadata":{},"cell_type":"markdown","source":"Let's train a different model for $4$ characters of our main characters list:"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_characters_list <- df_main_characters['Character'] %>% unique %>% as.list\nmain_characters_list <- main_characters_list$Character\nmain_characters_list %>% print","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"character.Text.Generator <- function(curr_character) {\n    \n    all.words <- all.words <- reduce((df_all %>% filter(Character == curr_character))['Words.List'] %>% unlist, c)\n\n    tokenizer <- text_tokenizer()\n    tokenizer$fit_on_texts(all.words)\n    \n    curr_train_data <- get.Character.Windows(curr_character, tokenizer)\n    \n    X_in <- curr_train_data$X\n    X_in <- X_in %>% array_reshape(c(dim(X_in)[1], dim(X_in)[2], 1))\n    \n    Y_in <- curr_train_data$Y\n    max_id = length(tokenizer$word_index)\n    Y_in <- to_categorical(Y_in - 1, num_classes = max_id)\n    \n    model <- gen.Model(max_id = max_id)\n    model %>% fit(x = X_in, y = Y_in,\n                  batch_size = batch.Size,\n                  epochs = n.Epochs,\n                  verbose = 1)\n    \n    return(model)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, I'm taking C3PO, Luke, Vader and Yoda!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training C3PO text generator')\nthreepio_model <- character.Text.Generator('threepio')\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Luke text generator...')\nluke_model <- character.Text.Generator('luke')\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Traning Vader text generator...')\nvader_model <- character.Text.Generator('vader')\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Yoda text generator...')\nyoda_model <- character.Text.Generator('yoda')\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's show a sentence for each case and take the final conclusions ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_window_list <- c('hello i\\'m a person that', 'well i don\\'t like to', 'why don\\'t we try to')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.A. C3PO\n![C3P0](https://hips.hearstapps.com/digitalspyuk.cdnds.net/16/46/1479397679-c-3po-see-threepio-68fe125c.jpeg?crop=0.501xw:1.00xh;0.301xw,0&resize=480:*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('C3PO Sentences:')\nlapply(initial_window_list, function(X) gen.Text(X, model=threepio_model)) %>% unlist %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.B. Luke Skywalker\n![Luke](https://s2.glbimg.com/LttsvVoQZGHoIJsmdlXMULY336A=/e.glbimg.com/og/ed/f/original/2019/09/23/ea1e16061bdf92edb111d8808c6741a6.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Luke Sentences:')\nlapply(initial_window_list, function(X) gen.Text(X, model=luke_model)) %>% unlist %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.C. Darth Vader\n![Vader](https://conteudo.imguol.com.br/c/entretenimento/81/2019/02/12/o-capacete-de-darth-vader-1550013937325_v2_900x506.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vader Sentences:')\nlapply(initial_window_list, function(X) gen.Text(X, model=vader_model)) %>% unlist %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.D. Master Yoda\n![Yoda](http://s2.glbimg.com/6Mt61D705hGBewAG7VNeI5hUjEg=/e.glbimg.com/og/ed/f/original/2015/09/01/yoda-the-empire-strikes-back.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Yoda Sentences:')\nlapply(initial_window_list, function(X) gen.Text(X, model=yoda_model)) %>% unlist %>% print","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Final Conclusions and Next Steps\n\nThe generated text were simple, but interesting. Generating texts with Neural Networks can be a complex task: the validation and data exploration may be different from normal problems - tools like word clouds or different types of plots and statistics may become useful here.\n\nWe can aways improve this kind of model: would it be possible to detect the end of sentences by reading the final dots? Would it be possible to predict who would be the next character that will speak? It would be really cool - it would be possible to develop a Star Wars **FULL SCRIPT** generator.\n\nThe sentences are not perfect at all, but the aim of this notebook is not to create a perfect text generator, it's just a simple tutorial for people who want to start to explore text processing models ;)\n\nCya!"},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\n\n- Thank you Jesús Martín de la Sierra (https://www.kaggle.com/jmartindelasierra), in the discussion of the link \"https://www.kaggle.com/questions-and-answers/113714\" you shown how to correct the Kaggle R bug that doesn't allow us to create Keras models in Notebooks."}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.4"}},"nbformat":4,"nbformat_minor":1}